LOGS:

Train on 60000 samples, validate on 10000 samples
Epoch 1/20

Epoch 00001: LearningRateScheduler setting learning rate to 0.01.
60000/60000 [==============================] - 14s 237us/step - loss: 0.2918 -
acc: 0.9336 - val_loss: 0.0615 - val_acc: 0.9803

Epoch 00001: val_acc improved from -inf to 0.98030, saving model to
model/weights.hdf5
Epoch 2/20

Epoch 00002: LearningRateScheduler setting learning rate to 0.0075815011.
60000/60000 [==============================] - 11s 184us/step - loss: 0.0923 -
acc: 0.9764 - val_loss: 0.0528 - val_acc: 0.9828

Epoch 00002: val_acc improved from 0.98030 to 0.98280, saving model to
model/weights.hdf5
Epoch 3/20

Epoch 00003: LearningRateScheduler setting learning rate to 0.0061050061.
60000/60000 [==============================] - 11s 184us/step - loss: 0.0703 -
acc: 0.9811 - val_loss: 0.0471 - val_acc: 0.9867

Epoch 00003: val_acc improved from 0.98280 to 0.98670, saving model to
model/weights.hdf5
Epoch 4/20

Epoch 00004: LearningRateScheduler setting learning rate to 0.005109862.
60000/60000 [==============================] - 11s 181us/step - loss: 0.0616 -
acc: 0.9829 - val_loss: 0.0291 - val_acc: 0.9909

Epoch 00004: val_acc improved from 0.98670 to 0.99090, saving model to
model/weights.hdf5
Epoch 5/20

Epoch 00005: LearningRateScheduler setting learning rate to 0.0043936731.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0525 -
acc: 0.9851 - val_loss: 0.0254 - val_acc: 0.9929

Epoch 00005: val_acc improved from 0.99090 to 0.99290, saving model to
model/weights.hdf5
Epoch 6/20

Epoch 00006: LearningRateScheduler setting learning rate to 0.0038535645.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0498 -
acc: 0.9854 - val_loss: 0.0277 - val_acc: 0.9917

Epoch 00006: val_acc did not improve from 0.99290
Epoch 7/20

Epoch 00007: LearningRateScheduler setting learning rate to 0.003431709.
60000/60000 [==============================] - 11s 182us/step - loss: 0.0465 -
acc: 0.9859 - val_loss: 0.0251 - val_acc: 0.9926

Epoch 00007: val_acc did not improve from 0.99290
Epoch 8/20

Epoch 00008: LearningRateScheduler setting learning rate to 0.0030931024.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0419 -
acc: 0.9882 - val_loss: 0.0226 - val_acc: 0.9926

Epoch 00008: val_acc did not improve from 0.99290
Epoch 9/20

Epoch 00009: LearningRateScheduler setting learning rate to 0.0028153153.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0396 -
acc: 0.9886 - val_loss: 0.0236 - val_acc: 0.9923

Epoch 00009: val_acc did not improve from 0.99290
Epoch 10/20

Epoch 00010: LearningRateScheduler setting learning rate to 0.0025833118.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0388 -
acc: 0.9883 - val_loss: 0.0228 - val_acc: 0.9927

Epoch 00010: val_acc did not improve from 0.99290
Epoch 11/20

Epoch 00011: LearningRateScheduler setting learning rate to 0.0023866348.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0375 -
acc: 0.9892 - val_loss: 0.0206 - val_acc: 0.9938

Epoch 00011: val_acc improved from 0.99290 to 0.99380, saving model to
model/weights.hdf5
Epoch 12/20

Epoch 00012: LearningRateScheduler setting learning rate to 0.0022177866.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0339 -
acc: 0.9902 - val_loss: 0.0224 - val_acc: 0.9935

Epoch 00012: val_acc did not improve from 0.99380
Epoch 13/20

Epoch 00013: LearningRateScheduler setting learning rate to 0.002071251.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0345 -
acc: 0.9898 - val_loss: 0.0231 - val_acc: 0.9934

Epoch 00013: val_acc did not improve from 0.99380
Epoch 14/20

Epoch 00014: LearningRateScheduler setting learning rate to 0.0019428793.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0346 -
acc: 0.9896 - val_loss: 0.0274 - val_acc: 0.9918

Epoch 00014: val_acc did not improve from 0.99380
Epoch 15/20

Epoch 00015: LearningRateScheduler setting learning rate to 0.0018294914.
60000/60000 [==============================] - 11s 185us/step - loss: 0.0331 -
acc: 0.9901 - val_loss: 0.0186 - val_acc: 0.9944

Epoch 00015: val_acc improved from 0.99380 to 0.99440, saving model to
model/weights.hdf5
Epoch 16/20

Epoch 00016: LearningRateScheduler setting learning rate to 0.0017286085.
60000/60000 [==============================] - 11s 184us/step - loss: 0.0295 -
acc: 0.9914 - val_loss: 0.0243 - val_acc: 0.9922

Epoch 00016: val_acc did not improve from 0.99440
Epoch 17/20

Epoch 00017: LearningRateScheduler setting learning rate to 0.00163827.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0303 -
acc: 0.9912 - val_loss: 0.0185 - val_acc: 0.9951

Epoch 00017: val_acc improved from 0.99440 to 0.9
Epoch 18/20

Epoch 00018: LearningRateScheduler setting learning rate to 0.0015569049.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0288 -
acc: 0.9911 - val_loss: 0.0175 - val_acc: 0.9947

Epoch 00018: val_acc did not improve from 0.99510
Epoch 19/20

Epoch 00019: LearningRateScheduler setting learning rate to 0.0014832394.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0286 -
acc: 0.9911 - val_loss: 0.0187 - val_acc: 0.9942

Epoch 00019: val_acc did not improve from 0.99510
Epoch 20/20

Epoch 00020: LearningRateScheduler setting learning rate to 0.00141623.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0285 -
acc: 0.9913 - val_loss: 0.0208 - val_acc: 0.9934

Epoch 00020: val_acc did not improve from 0.99510

OUTPUT
[0.018461870736838318, 0.9951]

My Strategy:
1. Since we know that for this dataset the Global Receptive Field should be 5
   x 5. I took care of that when designing the Network.
2. In the initial layers instead of 10 filters, 16 and 32 filters give better
   results. This is because the output has better features in this case
3. With the starting learning rate of 0.003, after about 10 epochs, the
   accuracy wasn't changing much. So I started the learning rate with 0.01 instead.
   This probably indicates that my model was getting stuck in a local minima.
4. The dropout in the last later did not make sense. Removing the same also
   helped with the accuracy

